{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.cluster import KMeans\n",
    "from sliding_window import sliding_window\n",
    "from cluster_eval import greedy_relabel\n",
    "from cluster_eval import brute_relabel\n",
    "from cluster_eval import print_results\n",
    "from cluster_eval import relabel_list\n",
    "from keras import layers\n",
    "\n",
    "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
    "NB_SENSOR_CHANNELS = 113\n",
    "\n",
    "# Hardcoded number of classes in the gesture recognition problem\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "\n",
    "# Length of the input sequence after convolutional operations\n",
    "FINAL_SEQUENCE_LENGTH = 6\n",
    "\n",
    "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Number filters convolutional layers\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "# Size filters convolutional layers\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "# Number of unit in the long short-term recurrent layers\n",
    "NUM_UNITS_LSTM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file oppChallenge_locomotion.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n",
      "(465668, 113)\n",
      "(94260, 113)\n",
      "{0: 231751, 1: 130506, 2: 88883, 3: 14528}\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_dataset('oppChallenge_locomotion.data')\n",
    "\n",
    "def remove_class(X_train, y_train, X_test, y_test, class_n):\n",
    "    uy_train = y_train[y_train != class_n]\n",
    "    uX_train = X_train[y_train != class_n]\n",
    "    uy_test = y_test[y_test != class_n]\n",
    "    uX_test = X_test[y_test != class_n]\n",
    "    \n",
    "    classes = set(y_train)\n",
    "    \n",
    "    for x in range(class_n + 1, len(classes)):\n",
    "        uy_train[uy_train == x] = x - 1\n",
    "        uy_test[uy_test == x] = x - 1\n",
    "        \n",
    "    return uX_train, uy_train, uX_test, uy_test\n",
    "\n",
    "# def delete_features():\n",
    "# RKN^, RKN_, BACK, HIP, R-SHOE, L-SHOE\n",
    "# 53 Features\n",
    "\n",
    "#features_delete = np.arange(6, 15)\n",
    "#features_delete = np.concatenate([features_delete, np.arange(21, 36)])\n",
    "#features_delete = np.concatenate([features_delete, np.arange(45, 81)])\n",
    "\n",
    "# 60 Features\n",
    "#features_delete = np.arange(0, 6)\n",
    "#features_delete = np.concatenate([features_delete, np.arange(15, 21)])\n",
    "#features_delete = np.concatenate([features_delete, np.arange(36, 45)])\n",
    "#features_delete = np.concatenate([features_delete, np.arange(81, 113)])\n",
    "\n",
    "# 9 Features (Phone)\n",
    "#features_delete = np.arange(0, 36)\n",
    "#features_delete = np.concatenate([features_delete, np.arange(45, 113)])\n",
    "\n",
    "#features_delete = np.concatenate([features_delete, np.arange(6, 113)])\n",
    "\n",
    "#X_train = np.delete(X_train, features_delete, 1)\n",
    "#X_test = np.delete(X_test, features_delete, 1)\n",
    "\n",
    "classes = [\"Null\", \"Stand\", \"Walk\" ,\"Sit\", \"Lie\"]\n",
    "\n",
    "# 0 - null | 1 - stand | 2 - walk | 3 - sit | 4 - lie\n",
    "\n",
    "X_train, y_train, X_test, y_test = remove_class(X_train, y_train, X_test, y_test, 0) # remove null\n",
    "classes.remove(\"Null\")\n",
    "\n",
    "# 0 - stand | 1 - walk | 2 - sit | 3 - lie\n",
    "\n",
    "#X_train, y_train, X_test, y_test = remove_class(X_train, y_train, X_test, y_test, 1) # remove walking\n",
    "#classes.remove(\"Walk\")\n",
    "\n",
    "#X_train, y_train, X_test, y_test = remove_class(X_train, y_train, X_test, y_test, 3) # remove walking\n",
    "#classes.remove(\"Lie\")\n",
    "\n",
    "#X_train, y_train, X_test, y_test = remove_class(X_train, y_train, X_test, y_test, 2) # remove walking\n",
    "#classes.remove(\"Sit\")\n",
    "\n",
    "# 0 - stand | 1 - sit | 2 - lie\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "label_dict = dict(zip(unique, counts))\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..after sliding window (testing): inputs (7854, 24, 113), targets (7854,)\n",
      "(38804, 24, 113)\n",
      "(7854, 24, 113)\n"
     ]
    }
   ],
   "source": [
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "\n",
    "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 24, 64)            45568     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 24, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 24, 64)            24832     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 24, 113)           7345      \n",
      "=================================================================\n",
      "Total params: 98,481\n",
      "Trainable params: 98,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "# lstm autoencoder doesnt need a sliding window, redudent data can lead to overfitting\n",
    "\n",
    "seed_n = 9\n",
    "seed(seed_n)\n",
    "tf.random.set_seed(seed_n)\n",
    "\n",
    "model = Sequential()\n",
    "# encoder\n",
    "model.add(LSTM(64, activation='relu', input_shape=(SLIDING_WINDOW_LENGTH, 113), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "#model.add(LSTM(16, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(SLIDING_WINDOW_LENGTH))\n",
    "# decoder\n",
    "#model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(113)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "330/330 [==============================] - 21s 54ms/step - loss: 0.0704 - val_loss: 0.0057\n",
      "Epoch 2/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 3/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 4/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0065 - val_loss: 0.0047\n",
      "Epoch 5/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 6/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 7/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 8/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 9/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0031 - val_loss: 0.0035\n",
      "Epoch 10/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 11/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 12/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 13/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 14/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 15/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0023 - val_loss: 0.0048\n",
      "Epoch 16/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 17/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 18/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 19/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 20/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 21/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 22/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 23/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 24/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 25/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 26/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 27/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 28/30\n",
      "330/330 [==============================] - 11s 34ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 29/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 30/30\n",
      "330/330 [==============================] - 11s 33ms/step - loss: 0.0018 - val_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212a53931f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "model.fit(X_train, X_train, epochs=30, batch_size=100, validation_split=0.15, callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rae24_12_s0_d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = keras.models.load_model('rae24_12_s0_d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder becomes output\n",
    "encoder = keras.Model(inputs=model.inputs, outputs=model.layers[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38804, 32)\n",
      "(7854, 32)\n"
     ]
    }
   ],
   "source": [
    "# get encoded data\n",
    "encoded_train = encoder.predict(X_train, verbose=0)\n",
    "encoded_test = encoder.predict(X_test, verbose=0)\n",
    "print(encoded_train.shape)\n",
    "print(encoded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:\n",
      "2.1525440216064453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import operator\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 11\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_pred = KMeans(n_clusters=NUMBER_OF_CLUSTERS, random_state=0).fit(encoded_train).predict(encoded_test)\n",
    "#y_pred = KMeans(n_clusters=NUMBER_OF_CLUSTERS, random_state=0).fit(encoded_train, sample_weight=sample_weight_train).predict(encoded_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time:\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Labels:\n",
      "[0, 0, 2, 0, 3, 0, 1, 0, 0, 2, 0]\n",
      "\n",
      "F1:\n",
      "0.7350264174165717\n",
      "\n",
      "Accuracy:\n",
      "0.679144385026738\n",
      "\n",
      "ARI:\n",
      "0.4149103716516561\n"
     ]
    }
   ],
   "source": [
    "uy_pred = y_pred.copy()\n",
    "clusters = list(set(y_pred))\n",
    "clusters.sort()\n",
    "\n",
    "relabels = brute_relabel(uy_pred, y_test)\n",
    "#relabels = greedy_relabel(uy_pred, y_test)\n",
    "#relabels = [0, 0, 3, 0, 0, 0, 0, 0, 1, 2, 3]\n",
    "\n",
    "uy_pred = relabel_list(uy_pred, clusters, relabels)\n",
    "print_results(uy_pred, y_test, relabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Labels:\n",
      "[0, 0, 0, 2, 0, 0, 0, 0, 3, 1, 3]\n",
      "\n",
      "F1:\n",
      "0.759999914759273\n",
      "\n",
      "Accuracy:\n",
      "0.6633562515915458\n",
      "\n",
      "ARI:\n",
      "0.4301472472581655\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "#checking specific lists\n",
    "\n",
    "clusters = list(set(y_pred))\n",
    "clusters.sort()\n",
    "\n",
    "f1 = 0\n",
    "label_list = None\n",
    "for test_list in test:\n",
    "    for i in range(0, len(test_list)):\n",
    "        score = metrics.f1_score(relabel_list(y_pred.copy(), clusters, list(test_list[i])), y_test, average='weighted')\n",
    "        if (score > f1):\n",
    "            f1 = score\n",
    "            label_list = list(test_list[i])\n",
    "\n",
    "uy_pred = y_pred.copy()\n",
    "\n",
    "uy_pred = relabel_list(uy_pred, clusters, label_list)\n",
    "print_results(uy_pred, y_test, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(set(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "test = []\n",
    "test.append(list(set(itertools.permutations([0, 1, 0, 0, 2, 0, 3, 0, 0, 3, 2]))))\n",
    "test.append(list(set(itertools.permutations([3, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0]))))\n",
    "test.append(list(set(itertools.permutations([0, 0, 1, 0, 0, 3, 0, 0, 3, 0, 2]))))\n",
    "#test.append(list(set(itertools.permutations([0, 0, 1, 0, 0, 3, 3, 0, 3, 0, 2]))))\n",
    "test.append(list(set(itertools.permutations([0, 0, 3, 0, 1, 0, 2, 0, 0, 0, 0]))))\n",
    "#test.append(list(set(itertools.permutations([1, 2, 0, 1, 2, 1, 0, 2, 3, 0, 0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(y_test, uy_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp = disp.plot()\n",
    "\n",
    "#plt.savefig('confusion_matricies/c4_f113_km' + str(NUMBER_OF_CLUSTERS) + '+sw24_12+rae.png') # number of classes, features, method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
