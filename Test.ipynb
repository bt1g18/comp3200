{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook presents the architecture of DeepConvLSTM: a deep framework for wearable activity recognition based on convolutional and LSTM recurrent units. To obtain a detailed description of the architecture consult the paper \"Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One of the benchmarks dataset employed to evaluate DeepConvLSTM is the 'OPPORTUNITY Activity Recognition Data Set'. OPPORTUNITY is a dataset devised to benchmark human activity recognition algorithms. It comprises the readings of motion sensors recorded while users executed typical daily activities and includes several annotations of gestures and modes of locomotion (visit https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition for further info). In this example DeepConvLSTM will perform recognition of sporadic gestures. This task concerns recognition of the different right-arm gestures. This is a 18 class segmentation and classification problem.\n",
    "\n",
    "The dataset must be be preprocessed prior to be feed to the neural network, in order to fill in missing values using linear interpolation and to do a per channel normalization to interval [0,1]. A Python script is provided to automatically preprocess the data, download the original OPPORTUNITY dataset if required and segment sensor data into train and test.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We would recommend to download the OPPORTUNITY zip file from the UCI repository and then use the script to generate the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess_data.py [-h] -i INPUT -o OUTPUT [-t {gestures,locomotion}]\r\n",
      "\r\n",
      "Preprocess OPPORTUNITY dataset\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT, --input INPUT\r\n",
      "                        OPPORTUNITY zip file\r\n",
      "  -o OUTPUT, --output OUTPUT\r\n",
      "                        Processed data file\r\n",
      "  -t {gestures,locomotion}, --task {gestures,locomotion}\r\n",
      "                        Type of activities to be recognized\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
      "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py -i data/OpportunityUCIDataset.zip -o oppChallenge_gestures.data -t gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DeepConvLSTM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DeepConvLSTM is defined as a neural netowrk which combines convolutional and recurrent layers. The convolutional\n",
    "layers act as feature extractors and provide abstract representations of the input sensor data in feature\n",
    "maps. The recurrent layers model the temporal dynamics of the activation of the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sliding_window import sliding_window\n",
    "from keras import layers\n",
    "\n",
    "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
    "NB_SENSOR_CHANNELS = 113\n",
    "\n",
    "# Hardcoded number of classes in the gesture recognition problem\n",
    "NUM_CLASSES = 18\n",
    "\n",
    "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "\n",
    "# Length of the input sequence after convolutional operations\n",
    "FINAL_SEQUENCE_LENGTH = 8\n",
    "\n",
    "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Number filters convolutional layers\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "# Size filters convolutional layers\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "# Number of unit in the long short-term recurrent layers\n",
    "NUM_UNITS_LSTM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the sensor data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load the OPPORTUNITY processed dataset. Sensor data is segmented using a sliding window of fixed length. The class associated with each segment corresponds to the gesture which has been observed during that interval. Given a sliding window of length T, we choose the class of the sequence as the label at t=T, or in other words, the label of last sample in the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file oppChallenge_locomotion.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n",
      "(465668, 113)\n",
      "(94260, 113)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import collections\n",
    "\n",
    "def load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_dataset('oppChallenge_locomotion.data')\n",
    "\n",
    "def remove_nulls(X_train, y_train, X_test, y_test):\n",
    "    uy_train = y_train[y_train > 0]\n",
    "    uX_train = X_train[y_train > 0]\n",
    "    uy_test = y_test[y_test > 0]\n",
    "    uX_test = X_test[y_test > 0]\n",
    "\n",
    "    uy_train[uy_train == 1] = 0\n",
    "    uy_train[uy_train == 2] = 1\n",
    "    uy_train[uy_train == 3] = 2\n",
    "    uy_train[uy_train == 4] = 3\n",
    "\n",
    "    uy_test[uy_test == 1] = 0\n",
    "    uy_test[uy_test == 2] = 1\n",
    "    uy_test[uy_test == 3] = 2\n",
    "    uy_test[uy_test == 4] = 3\n",
    "    \n",
    "    return uX_train, uy_train, uX_test, uy_test\n",
    "\n",
    "\n",
    "# RKN^, RKN_, BACK, HIP, R-SHOE, L-SHOE\n",
    "# 53 Features\n",
    "\n",
    "#features_delete = np.arange(6, 15) # 6,15\n",
    "#features_delete = np.concatenate([features_delete, np.arange(6, 113)])\n",
    "#features_delete = np.concatenate([features_delete, np.arange(21, 36)])\n",
    "#features_delete = np.concatenate([features_delete, np.arange(45, 81)])\n",
    "\n",
    "#X_train = np.delete(X_train, features_delete, 1)\n",
    "#X_test = np.delete(X_test, features_delete, 1)\n",
    "\n",
    "X_train, y_train, X_test, y_test = remove_nulls(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#unique, counts = np.unique(y_train, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..after sliding window (testing): inputs (7854, 24, 113), targets (7854,)\n",
      "(38804, 24, 113)\n",
      "(7854, 24, 113)\n"
     ]
    }
   ],
   "source": [
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "\n",
    "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "#X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "#X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
    "\n",
    "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
    "#X_train = X_train.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\n",
    "#X_test = X_test.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38804, 113)\n",
      "(7854, 113)\n"
     ]
    }
   ],
   "source": [
    "def mean_sliding_window(X):\n",
    "    rows = X.shape[0]\n",
    "    n_features = X.shape[2]\n",
    "    window_mean = np.zeros((rows, n_features))\n",
    "    for i in range(rows):\n",
    "        window_mean[i] = np.mean(X_train[i],axis=0).reshape(1, n_features)\n",
    "    return window_mean\n",
    "\n",
    "s_X_train = mean_sliding_window(X_train)\n",
    "s_X_test = mean_sliding_window(X_test)\n",
    "\n",
    "print(s_X_train.shape)\n",
    "print(s_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Lasagne network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sensor data are processed by four convolutional layer which allow to learn features from the data. Two dense layers then perform a non-lineartransformation which yields the classification outcome with a softmax logistic regresion output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = {}\n",
    "net['input'] = lasagne.layers.InputLayer((BATCH_SIZE, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\n",
    "net['conv1/5x1'] = lasagne.layers.Conv2DLayer(net['input'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "net['conv2/5x1'] = lasagne.layers.Conv2DLayer(net['conv1/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "net['conv3/5x1'] = lasagne.layers.Conv2DLayer(net['conv2/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "net['conv4/5x1'] = lasagne.layers.Conv2DLayer(net['conv3/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "net['shuff'] = lasagne.layers.DimshuffleLayer(net['conv4/5x1'], (0, 2, 1, 3))\n",
    "net['lstm1'] = lasagne.layers.LSTMLayer(net['shuff'], NUM_UNITS_LSTM)\n",
    "net['lstm2'] = lasagne.layers.LSTMLayer(net['lstm1'], NUM_UNITS_LSTM)\n",
    "# In order to connect a recurrent layer to a dense layer, it is necessary to flatten the first two dimensions\n",
    "# to cause each time step of each sequence to be processed independently (see Lasagne docs for further information)\n",
    "net['shp1'] = lasagne.layers.ReshapeLayer(net['lstm2'], (-1, NUM_UNITS_LSTM))\n",
    "net['prob'] = lasagne.layers.DenseLayer(net['shp1'],NUM_CLASSES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "# Tensors reshaped back to the original shape\n",
    "net['shp2'] = lasagne.layers.ReshapeLayer(net['prob'], (BATCH_SIZE, FINAL_SEQUENCE_LENGTH, NUM_CLASSES))\n",
    "# Last sample in the sequence is considered\n",
    "net['output'] = lasagne.layers.SliceLayer(net['shp2'], -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is populated with the weights of the pretrained network\n",
    "\n",
    "# all_params_values = pd.read_pickle(r'weights/DeepConvLSTM_oppChallenge_gestures.pkl') - old format of pickle.dump()\n",
    "all_params_values = pickle.load(open('weights.pkl', 'rb'))\n",
    "\n",
    "# save weights as pickle - pickle.dump(all_params_values ,open('weights.pkl' , 'wb' ))\n",
    "\n",
    "lasagne.layers.set_all_param_values(net['output'], all_params_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compile the Theano function required to classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation of theano functions\n",
    "# Obtaining the probability distribution over classes\n",
    "test_prediction = lasagne.layers.get_output(net['output'], deterministic=True)\n",
    "# Returning the predicted output for the given minibatch\n",
    "test_fn =  theano.function([ net['input'].input_var], [T.argmax(test_prediction, axis=1)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Testing data are segmented in minibatches and classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1654 instances in mini-batches of 100\n"
     ]
    }
   ],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "# Classification of the testing data\n",
    "print(\"Processing {0} instances in mini-batches of {1}\".format(X_test.shape[0],BATCH_SIZE))\n",
    "test_pred = np.empty((0))\n",
    "test_true = np.empty((0))\n",
    "start_time = time.time()\n",
    "for batch in iterate_minibatches(X_test, y_test, BATCH_SIZE):\n",
    "    inputs, targets = batch\n",
    "    y_pred, = test_fn(inputs)\n",
    "    test_pred = np.append(test_pred, y_pred, axis=0)\n",
    "    test_true = np.append(test_true, targets, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Models is evaluated using the F-Measure, a measure that considers the correct classification of each class equally important. Class imbalance is countered by weighting classes according to their sample proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Results||\n",
      "\tTook 28.824s.\n",
      "\tTest fscore:\t0.5317 \n"
     ]
    }
   ],
   "source": [
    "# Results presentation\n",
    "print(\"||Results||\")\n",
    "print(\"\\tTook {:.3f}s.\".format( time.time() - start_time))\n",
    "import sklearn.metrics as metrics\n",
    "print(\"\\tTest fscore:\\t{:.4f} \".format(metrics.f1_score(test_true, test_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with sliding window\n",
    "# window is used to refit kmeans to change the positioning of the centroids\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "size = 1000\n",
    "\n",
    "asdf1 = X_train[:size,:,:]\n",
    "asdf2 = X_test[:size,:,:]\n",
    "asdf3 = y_test[:size]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=3425)\n",
    "for window in range(X_train.shape[0]):\n",
    "    kmeans.fit(X_train[window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7854 instances in mini-batches of 100\n"
     ]
    }
   ],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "def mostFrequent(arr, n): \n",
    "    # Sort the array \n",
    "    arr.sort() \n",
    "    # find the max frequency using \n",
    "    # linear traversal \n",
    "    max_count = 1; res = arr[0]; curr_count = 1\n",
    "    for i in range(1, n):  \n",
    "        if (arr[i] == arr[i - 1]): \n",
    "            curr_count += 1\n",
    "        else: \n",
    "            if (curr_count > max_count):  \n",
    "                max_count = curr_count \n",
    "                res = arr[i - 1]          \n",
    "            curr_count = 1\n",
    "    # If last element is most frequent \n",
    "    if (curr_count > max_count): \n",
    "        max_count = curr_count \n",
    "        res = arr[n - 1] \n",
    "    return res \n",
    "\n",
    "        \n",
    "# Classification of the testing data\n",
    "print(\"Processing {0} instances in mini-batches of {1}\".format(X_test.shape[0], BATCH_SIZE))\n",
    "test_pred = np.empty((0))\n",
    "test_true = np.empty((0))\n",
    "start_time = time.time()\n",
    "for batch in iterate_minibatches(X_test, y_test, BATCH_SIZE):\n",
    "    inputs, targets = batch\n",
    "    # sliding window inputs are reduced to one dimension and then predicted\n",
    "    y_pred = np.zeros((BATCH_SIZE,))\n",
    "    for window in range(inputs.shape[0]):\n",
    "        labels = kmeans.predict(inputs[window])\n",
    "        label = mostFrequent(labels, inputs[window].shape[0]) # take mode of prediction\n",
    "        y_pred[window] = label\n",
    "    test_pred = np.append(test_pred, y_pred, axis=0)\n",
    "    test_true = np.append(test_true, targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09219750230878679\n"
     ]
    }
   ],
   "source": [
    "print(metrics.adjusted_rand_score(test_pred, test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31033466772971036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(X_train) # 5 with null & 4 without null\n",
    "# gmm = GaussianMixture(n_components = 4, covariance_type='full').fit(X_train) # change number of iterations cause it didn't converge\n",
    "\n",
    "y_pred = kmeans.predict(X_test)\n",
    "#y_pred = spectral.fit_predict(X_test)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:    (94260, 113)\n",
      "Transformed shape: (94260, 80)\n",
      "0.3699421732152517\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_test)\n",
    "# print('at', x, '% of the variance ======')\n",
    "print(\"Original shape:   \", X_test.shape)\n",
    "print(\"Transformed shape:\", X_pca.shape)\n",
    "\n",
    "#df_redd = pd.DataFrame(X_pca)\n",
    "#X_traind, X_testd, y_traind, y_testd = train_test_split(df_redd, y_train)\n",
    "\n",
    "clf = KMeans(n_clusters=4, random_state=0).fit(X_pca)\n",
    "\n",
    "y_pred = clf.predict(X_pca)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 1, 24, 113)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 1, 24, 102)        103836    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 1, 12, 102)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 1, 12, 86)         79034     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 1, 6, 86)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 1, 6, 64)          49600     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 1, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 1, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_37 (UpSampling (None, 2, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 2, 6, 86)          49622     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_38 (UpSampling (None, 4, 12, 86)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 2, 10, 102)        79050     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_39 (UpSampling (None, 4, 20, 102)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 4, 20, 1)          919       \n",
      "=================================================================\n",
      "Total params: 398,989\n",
      "Trainable params: 398,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sliding_window = keras.Input(shape=(30, 113))\n",
    "\n",
    "x = layers.Conv2D(112, (3, 3), activation='relu', padding='same')(sliding_window)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = layers.Conv2D(112, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(sliding_window, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_17 (Conv1D)           (None, 2, 64)             512       \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1, 32)             14368     \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_23 (Conv1DT (None, 15, 32)            7200      \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_24 (Conv1DT (None, 225, 64)           14400     \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_25 (Conv1DT (None, 225, 1)            449       \n",
      "=================================================================\n",
      "Total params: 36,929\n",
      "Trainable params: 36,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder1D = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(30, 113)),\n",
    "        layers.Conv1D(filters=64, kernel_size=7, padding=\"same\", strides=15, activation=\"relu\"),\n",
    "        #layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(filters=32, kernel_size=7, padding=\"same\", strides=15, activation=\"relu\"),\n",
    "        layers.Conv1DTranspose(filters=32, kernel_size=7, padding=\"same\", strides=15, activation=\"relu\"),\n",
    "        #layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(filters=64, kernel_size=7, padding=\"same\", strides=15, activation=\"relu\"),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "autoencoder1D.compile(optimizer='adam', loss=\"mse\")\n",
    "autoencoder1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [None, 30, 113]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-33e65670babf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m autoencoder1D.fit(X_train, X_train,\n\u001b[0m\u001b[0;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\ben_t\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [None, 30, 113]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "\n",
    "autoencoder1D.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 108)            95904     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 92)                73968     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 86)                7998      \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 30, 86)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 92)            65872     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 108)           86832     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 113)           12317     \n",
      "=================================================================\n",
      "Total params: 342,891\n",
      "Trainable params: 342,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "# should the autoencoder reduce the size of the sliding window?\n",
    "# lstm autoencoder doesnt need a sliding window, redudent data can lead to overfitting\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(108, activation='relu', input_shape=(30, 113), return_sequences=True))\n",
    "model.add(LSTM(92, activation='relu', return_sequences=False))\n",
    "model.add(Dense(86))\n",
    "model.add(RepeatVector(30))\n",
    "model.add(LSTM(92, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(108, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(113)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  2/311 [..............................] - ETA: 10:08 - loss: 0.0116WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1257s vs `on_train_batch_end` time: 3.8118s). Check your callbacks.\n",
      "311/311 [==============================] - 43s 139ms/step - loss: 0.0057\n",
      "Epoch 2/10\n",
      "311/311 [==============================] - 40s 128ms/step - loss: 0.0037\n",
      "Epoch 3/10\n",
      "311/311 [==============================] - 39s 126ms/step - loss: 0.0030\n",
      "Epoch 4/10\n",
      "311/311 [==============================] - 39s 124ms/step - loss: 0.0027\n",
      "Epoch 5/10\n",
      "311/311 [==============================] - 38s 121ms/step - loss: 0.0024\n",
      "Epoch 6/10\n",
      "311/311 [==============================] - 39s 124ms/step - loss: 0.0023\n",
      "Epoch 7/10\n",
      "311/311 [==============================] - 40s 129ms/step - loss: 0.0021\n",
      "Epoch 8/10\n",
      "311/311 [==============================] - 39s 124ms/step - loss: 0.0020\n",
      "Epoch 9/10\n",
      "311/311 [==============================] - 39s 127ms/step - loss: 0.0020\n",
      "Epoch 10/10\n",
      "311/311 [==============================] - 40s 129ms/step - loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e8574f250>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "asdf_T = X_train[:100,:,:]\n",
    "\n",
    "model.fit(X_train, X_train, epochs=10, batch_size=100, callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder becomes output\n",
    "encoder = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31043, 86)\n"
     ]
    }
   ],
   "source": [
    "# get encoded data\n",
    "encoded_train = encoder.predict(X_train, verbose=0)\n",
    "encoded_test = encoder.predict(X_test, verbose=0)\n",
    "print(encoded_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6283, 2880)\n"
     ]
    }
   ],
   "source": [
    "#flattened_e_train = np.reshape(encoded_train,(encoded_train.shape[0],encoded_train.shape[1]*encoded_train.shape[2]))\n",
    "#flattened_e_test = np.reshape(encoded_test,(encoded_test.shape[0],encoded_test.shape[1]*encoded_test.shape[2]))\n",
    "print(flattened_encoded.shape)\n",
    "\n",
    "flattened_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "flattened_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34250344732721577\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(encoded_train) # 5 with null & 4 without null\n",
    "\n",
    "y_pred = kmeans.predict(encoded_test)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_pred, y_test.reshape(y_test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3174531239665948\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(encoded_train) # 5 with null & 4 without null\n",
    "\n",
    "y_pred = kmeans.predict(encoded_test)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_pred, y_test.reshape(y_test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
